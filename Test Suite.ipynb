{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "32b65c7f-c1f5-48a6-8af5-2d923929166b",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## Imports and Notebook Runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5dbb1d2-2476-40f5-84fa-4ffd9c31f29b",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.livy.statement-meta+json": {
       "execution_finish_time": "2025-08-18T18:36:00.6563252Z",
       "execution_start_time": "2025-08-18T18:35:58.2321025Z",
       "livy_statement_state": "available",
       "normalized_state": "finished",
       "parent_msg_id": "9c19b333-eeea-4d6f-9b14-752c0af8b4ea",
       "queued_time": "2025-08-18T18:35:47.0875966Z",
       "session_id": "099b5ced-d358-4fcb-b3a5-5180b5324e70",
       "session_start_time": "2025-08-18T18:35:47.0885483Z",
       "spark_pool": null,
       "state": "finished",
       "statement_id": 3,
       "statement_ids": [
        3
       ]
      },
      "text/plain": [
       "StatementMeta(, 099b5ced-d358-4fcb-b3a5-5180b5324e70, 3, Finished, Available, Finished)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from abc import ABC, abstractmethod\n",
    "from typing import Dict, List, Optional, Any\n",
    "from datetime import datetime\n",
    "from pyspark.sql import DataFrame, SparkSession, Column\n",
    "from pyspark.sql import functions as F\n",
    "from pandas import DataFrame as pd_DataFrame\n",
    "# Common imports double imported for clarity and ease of use\n",
    "from pyspark.sql.functions import col, lit "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bea4dd29-a1db-4f14-a21a-0d448982eb1b",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## **Core Test Suite Logic**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "889dc907-032e-4376-884b-ded3c32f7b21",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "### Helper Classes and Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "74a42ea4-75e3-48e5-bf99-8d74389ccb31",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.livy.statement-meta+json": {
       "execution_finish_time": "2025-08-18T18:36:01.0745044Z",
       "execution_start_time": "2025-08-18T18:36:00.6586312Z",
       "livy_statement_state": "available",
       "normalized_state": "finished",
       "parent_msg_id": "9ef4afc0-ab5f-4100-82c4-81e18e22b43e",
       "queued_time": "2025-08-18T18:35:47.090088Z",
       "session_id": "099b5ced-d358-4fcb-b3a5-5180b5324e70",
       "session_start_time": null,
       "spark_pool": null,
       "state": "finished",
       "statement_id": 4,
       "statement_ids": [
        4
       ]
      },
      "text/plain": [
       "StatementMeta(, 099b5ced-d358-4fcb-b3a5-5180b5324e70, 4, Finished, Available, Finished)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "class TestResult:\n",
    "    \"\"\"Immutable test result object\"\"\"\n",
    "    def __init__(self, test_name: str, table_name: str, passed: bool,\n",
    "                 error_count: int = 0, total_count: int = 0,\n",
    "                 details: str = \"\", execution_time: float = 0.0,\n",
    "                 sample_mode: bool = False, timestamp: Optional[datetime] = None, example_failures: Optional[pd_DataFrame] = None):\n",
    "        self.test_name = test_name\n",
    "        self.table_name = table_name\n",
    "        self.passed = passed\n",
    "        self.error_count = error_count\n",
    "        self.total_count = total_count\n",
    "        self.success_rate = (total_count - error_count) / total_count if total_count > 0 else 1.0\n",
    "        self.details = details\n",
    "        self.execution_time_seconds = execution_time\n",
    "        self.timestamp = timestamp or datetime.now()\n",
    "        self.sample_mode = sample_mode\n",
    "        self._example_failures = example_failures\n",
    "\n",
    "    def to_dict(self) -> Dict:\n",
    "        return {\n",
    "            'test_name': self.test_name,\n",
    "            'table_name': self.table_name,\n",
    "            'passed': self.passed,\n",
    "            'error_count': self.error_count,\n",
    "            'total_count': self.total_count,\n",
    "            'success_rate': self.success_rate,\n",
    "            'details': self.details,\n",
    "            'execution_time_seconds': self.execution_time_seconds,\n",
    "            'timestamp': self.timestamp,\n",
    "            'sample_mode': self.sample_mode,\n",
    "            # serialize to records for Spark/JSON friendliness\n",
    "            \"failures\": (\n",
    "                self._example_failures.to_dict(orient=\"records\")\n",
    "                if self._example_failures is not None else None\n",
    "            ),\n",
    "        }\n",
    "\n",
    "class TestResultBuilder:\n",
    "    \"\"\"Builder for creating TestResult objects incrementally\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self._test_name: Optional[str] = None\n",
    "        self._table_name: Optional[str] = None\n",
    "        self._passed: Optional[bool] = None\n",
    "        self._error_count: int = 0\n",
    "        self._total_count: int = 0\n",
    "        self._details: str = \"\"\n",
    "        self._execution_time: float = 0.0\n",
    "        self._sample_mode: bool = False\n",
    "        self._start_time: Optional[datetime] = None\n",
    "        self._example_failures: Optional[pd_DataFrame] = None\n",
    "    \n",
    "    def test_name(self, name: str) -> 'TestResultBuilder':\n",
    "        \"\"\"Set the test name\"\"\"\n",
    "        self._test_name = name\n",
    "        return self\n",
    "    \n",
    "    def table_name(self, name: str) -> 'TestResultBuilder':\n",
    "        \"\"\"Set the table name\"\"\"\n",
    "        self._table_name = name\n",
    "        return self\n",
    "    \n",
    "    def start_timing(self) -> 'TestResultBuilder':\n",
    "        \"\"\"Start timing the test execution\"\"\"\n",
    "        self._start_time = datetime.now()\n",
    "        return self\n",
    "    \n",
    "    def end_timing(self) -> 'TestResultBuilder':\n",
    "        \"\"\"End timing and calculate execution time\"\"\"\n",
    "        if self._start_time:\n",
    "            self._execution_time = (datetime.now() - self._start_time).total_seconds()\n",
    "        return self\n",
    "    \n",
    "    def counts(self, total: int, errors: int = 0) -> 'TestResultBuilder':\n",
    "        \"\"\"Set total and error counts\"\"\"\n",
    "        self._total_count = total\n",
    "        self._error_count = errors\n",
    "        return self\n",
    "    \n",
    "    def passed(self, is_passed: bool) -> 'TestResultBuilder':\n",
    "        \"\"\"Set whether the test passed\"\"\"\n",
    "        self._passed = is_passed\n",
    "        return self\n",
    "    \n",
    "    def details(self, details: str) -> 'TestResultBuilder':\n",
    "        \"\"\"Set test details/description\"\"\"\n",
    "        self._details = details\n",
    "        return self\n",
    "    \n",
    "    def sample_mode(self, is_sample: bool) -> 'TestResultBuilder':\n",
    "        \"\"\"Set whether test was run in sample mode\"\"\"\n",
    "        self._sample_mode = is_sample\n",
    "        return self\n",
    "    \n",
    "    def cache_failure(\n",
    "        self,\n",
    "        failures_df: DataFrame,\n",
    "        limit_rows: int = 5,\n",
    "        sample_cols: Optional[list[str]] = None,\n",
    "        max_bytes: int = 256_000,  # ~250 KB guard\n",
    "    ) -> \"TestResultBuilder\":\n",
    "        \"\"\"Materialize a tiny, driver-safe sample of failing rows as pandas.\"\"\"\n",
    "        df = failures_df\n",
    "        if sample_cols:\n",
    "            # only keep requested columns if present\n",
    "            keep = [c for c in sample_cols if c in df.columns]\n",
    "            if keep:\n",
    "                df = df.select(*keep)\n",
    "        pdf = df.limit(limit_rows).toPandas()\n",
    "\n",
    "        # crude size guard\n",
    "        if pdf.memory_usage(index=True, deep=True).sum() > max_bytes:\n",
    "            # trim to first N columns to stay under the cap\n",
    "            max_cols = max(1, int(len(pdf.columns) / 2))\n",
    "            pdf = pdf.iloc[:, :max_cols]\n",
    "\n",
    "        self._example_failures = pdf\n",
    "        return self\n",
    "    def success(self, total: int, details: str = \"\") -> 'TestResultBuilder':\n",
    "        \"\"\"Convenience method for successful test\"\"\"\n",
    "        return self.counts(total, 0).passed(True).details(details)\n",
    "    \n",
    "    def failure(self, total: int, errors: int, details: str = \"\") -> 'TestResultBuilder':\n",
    "        \"\"\"Convenience method for failed test\"\"\"\n",
    "        return self.counts(total, errors).passed(False).details(details)\n",
    "    \n",
    "    def exception(self, error: Exception, details: str = \"\") -> 'TestResultBuilder':\n",
    "        \"\"\"Convenience method for test that threw exception\"\"\"\n",
    "        error_details = f\"{details} Exception: {str(error)}\" if details else f\"Exception: {str(error)}\"\n",
    "        return self.counts(-1, -1).passed(False).details(error_details)\n",
    "    \n",
    "\n",
    "    def auto_pass_fail(self, total: int, errors: int, details: str = \"\") -> 'TestResultBuilder':\n",
    "        \"\"\"Automatically set pass/fail based on error count\"\"\"\n",
    "        is_passed = errors == 0\n",
    "        return self.counts(total, errors).passed(is_passed).details(details)\n",
    "    \n",
    "    def auto_build(self, total: int, errors: int, details: str = \"\") -> TestResult:\n",
    "        \"\"\"Automatically set pass/fail and build test result\"\"\"\n",
    "        return self.auto_pass_fail(total, errors, details).end_timing().build()\n",
    "    @classmethod\n",
    "    def for_test(cls, test_name: str, table_name: str, sample_mode: bool = False) -> 'TestResultBuilder':\n",
    "        \"\"\"Factory method to create a builder with common initial setup\"\"\"\n",
    "        return (cls()\n",
    "                .test_name(test_name)\n",
    "                .table_name(table_name)\n",
    "                .sample_mode(sample_mode)\n",
    "                .start_timing())\n",
    "    \n",
    "    def build(self) -> TestResult:\n",
    "        \"\"\"Build the final test result object\"\"\"\n",
    "        if self._test_name is None:\n",
    "            raise ValueError(\"Test name is required\")\n",
    "        if self._table_name is None:\n",
    "            raise ValueError(\"Table name is required\")\n",
    "        if self._passed is None:\n",
    "            raise ValueError(\"Test result (passed/failed) is required\")\n",
    "        \n",
    "        return TestResult(\n",
    "            test_name=self._test_name,\n",
    "            table_name=self._table_name,\n",
    "            passed=self._passed,\n",
    "            error_count=self._error_count,\n",
    "            total_count=self._total_count,\n",
    "            details=self._details,\n",
    "            execution_time=self._execution_time,\n",
    "            sample_mode=self._sample_mode,\n",
    "            example_failures = self._example_failures\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "df032de0-527a-4ed6-95e6-123335df25d3",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.livy.statement-meta+json": {
       "execution_finish_time": "2025-08-18T18:36:01.427046Z",
       "execution_start_time": "2025-08-18T18:36:01.0769373Z",
       "livy_statement_state": "available",
       "normalized_state": "finished",
       "parent_msg_id": "2c2cac33-ddaa-4059-ad04-4a2d6662e06c",
       "queued_time": "2025-08-18T18:35:47.0920692Z",
       "session_id": "099b5ced-d358-4fcb-b3a5-5180b5324e70",
       "session_start_time": null,
       "spark_pool": null,
       "state": "finished",
       "statement_id": 5,
       "statement_ids": [
        5
       ]
      },
      "text/plain": [
       "StatementMeta(, 099b5ced-d358-4fcb-b3a5-5180b5324e70, 5, Finished, Available, Finished)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# TODO: Document the interaction between TestStrategy TestBuilder and TestResult\n",
    "class TestStrategy(ABC):\n",
    "    \"\"\"Abstract base class for all test strategies\"\"\"\n",
    "\n",
    "    @abstractmethod\n",
    "    def execute(self, df: DataFrame, *args, **kwargs) -> TestResult:\n",
    "        \"\"\"Execute the test and return a TestResult\"\"\"\n",
    "        pass\n",
    "    \n",
    "    @property\n",
    "    @abstractmethod\n",
    "    def test_name(self) -> str:\n",
    "        \"Returns the name of this test\"\n",
    "        pass\n",
    "\n",
    "class DataProvider:\n",
    "    \"\"\"Handles data access and filtering logic\"\"\"\n",
    "\n",
    "    def __init__(self, spark: SparkSession, lakehouse_name: str):\n",
    "        self.spark = spark\n",
    "        self.lakehouse_name = lakehouse_name\n",
    "        self.sample_mode = False\n",
    "        self.sample_properties = []\n",
    "    \n",
    "    def enable_sample_mode(self, sample_size: int = 10, random_seed: Optional[int] = None) -> List[str]:\n",
    "        \"\"\"Enable sample mode by randomly selecting properties\"\"\"\n",
    "        print(f'Enabling sample mode with {sample_size} properties...')\n",
    "        \n",
    "        # Get total count first\n",
    "        total_properties = self.spark.sql(f'SELECT COUNT(*) as cnt FROM {self.lakehouse_name}.DIM_PROPERTY').collect()[0]['cnt']\n",
    "        \n",
    "        if sample_size >= total_properties:\n",
    "            print(f'Requested sample size ({sample_size}) >= total properties ({total_properties}). Using all properties.')\n",
    "            sampled_properties_df = self.spark.sql(f'SELECT Property_UUID FROM {self.lakehouse_name}.DIM_PROPERTY')\n",
    "            self.sample_properties = [row['Property_UUID'] for row in sampled_properties_df.collect()]\n",
    "        else:\n",
    "            # Use orderBy(rand()) and limit for exact sample size\n",
    "            if random_seed is not None:\n",
    "                sampled_properties_df = (self.spark.sql(f'SELECT Property_UUID FROM {self.lakehouse_name}.DIM_PROPERTY')\n",
    "                                        .orderBy(F.rand(random_seed))\n",
    "                                        .limit(sample_size))\n",
    "            else:\n",
    "                sampled_properties_df = (self.spark.sql(f'SELECT Property_UUID FROM {self.lakehouse_name}.DIM_PROPERTY')\n",
    "                                        .orderBy(F.rand())\n",
    "                                        .limit(sample_size))\n",
    "            \n",
    "            self.sample_properties = [row['Property_UUID'] for row in sampled_properties_df.collect()]\n",
    "        \n",
    "        self.sample_mode = True\n",
    "        actual_count = len(self.sample_properties)\n",
    "        \n",
    "        print(f'Sample mode enabled with {actual_count} properties')\n",
    "        return self.sample_properties\n",
    "    \n",
    "    def disable_sample_mode(self):\n",
    "        \"\"\"Disables sample mode\"\"\"\n",
    "        self.sample_mode = False\n",
    "        self.sample_properties = []\n",
    "        print(\"Sample mode disabled - tests will run on full dataset\")\n",
    "    \n",
    "    def get_dataframe(self, table_name: str, filter_column: str = \"PROPERTY_UUID\", disable_sampling: bool = False) -> DataFrame:\n",
    "        df = self.spark.table(f\"{self.lakehouse_name}.{table_name}\")\n",
    "        if disable_sampling or not self.sample_mode or not self.sample_properties:\n",
    "            return df\n",
    "        if filter_column not in df.columns:\n",
    "            print(f\"‚ö†Ô∏è  Sampling disabled for {table_name}: no '{filter_column}' column.\")\n",
    "            return df\n",
    "        return df.filter(F.col(filter_column).isin(self.sample_properties))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dc85f8c-7b19-453c-ab8b-f0c792de371a",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "### `TestRunner` object: Manages the test queue and pretty prints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f994814-d84f-4896-b4c1-6a36857d3719",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.livy.statement-meta+json": {
       "execution_finish_time": "2025-08-18T18:36:01.8313459Z",
       "execution_start_time": "2025-08-18T18:36:01.4295597Z",
       "livy_statement_state": "available",
       "normalized_state": "finished",
       "parent_msg_id": "aee96097-dd46-406a-84d1-e9e79c9583b2",
       "queued_time": "2025-08-18T18:35:47.0941195Z",
       "session_id": "099b5ced-d358-4fcb-b3a5-5180b5324e70",
       "session_start_time": null,
       "spark_pool": null,
       "state": "finished",
       "statement_id": 6,
       "statement_ids": [
        6
       ]
      },
      "text/plain": [
       "StatementMeta(, 099b5ced-d358-4fcb-b3a5-5180b5324e70, 6, Finished, Available, Finished)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "class TestRunner:\n",
    "    \"\"\"Orchestrates test execution and result collection\"\"\"\n",
    "    \n",
    "    def __init__(self, data_provider: DataProvider):\n",
    "        self.data_provider = data_provider\n",
    "        self.test_results: List[TestResult] = []\n",
    "        self.queued_tests: List[Dict[str, Any]] = []\n",
    "    \n",
    "    def add_test(self, test_strategy: TestStrategy, table_name: str, \n",
    "                 filter_column: str = \"PROPERTY_UUID\", disable_sampling: bool = False, **test_kwargs):\n",
    "        \"\"\"Add a test to the execution queue\"\"\"\n",
    "\n",
    "        # If sample_mode is unspecified, use the data provider's default\n",
    "        \n",
    "        test_config = {\n",
    "            'strategy': test_strategy,\n",
    "            'table_name': table_name,\n",
    "            'filter_column': filter_column,\n",
    "            'disable_sampling': disable_sampling,\n",
    "            'kwargs': test_kwargs\n",
    "        }\n",
    "        self.queued_tests.append(test_config)\n",
    "        print(f'Added test: {test_strategy.test_name} for {table_name} to queue')\n",
    "    \n",
    "    def clear_queue(self):\n",
    "        \"\"\"Clear the test queue\"\"\"\n",
    "        self.queued_tests = []\n",
    "        self.test_results = []\n",
    "        print('Test queue cleared')\n",
    "    \n",
    "    def run_all_tests(self) -> List[TestResult]:\n",
    "        \"\"\"Execute all queued tests\"\"\"\n",
    "        print(\"=\"*60)\n",
    "        print(\"STARTING LAKEHOUSE DATA TESTING SUITE\")\n",
    "        print(f\"Mode: {'SAMPLE' if self.data_provider.sample_mode else 'FULL DATASET'}\")\n",
    "        if self.data_provider.sample_mode:\n",
    "            print(f\"Sample Properties: {len(self.data_provider.sample_properties)}\")\n",
    "        print(f\"Queued Tests: {len(self.queued_tests)}\")\n",
    "        print(\"=\"*60)\n",
    "        \n",
    "        self.test_results = []\n",
    "        \n",
    "        for test_config in self.queued_tests:\n",
    "            try:\n",
    "                strategy = test_config['strategy']\n",
    "                table_name = test_config['table_name']\n",
    "                filter_column = test_config['filter_column']\n",
    "                disable_sampling = test_config['disable_sampling']\n",
    "                test_kwargs = test_config['kwargs']\n",
    "                \n",
    "                print(f\"\\nüîÑ Running {strategy.test_name} on {table_name}...\")\n",
    "                \n",
    "                df = self.data_provider.get_dataframe(table_name, filter_column, disable_sampling)\n",
    "                result = strategy.execute(\n",
    "                    df, \n",
    "                    table_name=table_name, \n",
    "                    sample_mode=self.data_provider.sample_mode,\n",
    "                    **test_kwargs\n",
    "                )\n",
    "                self.test_results.append(result)\n",
    "                \n",
    "                status = \"‚úÖ PASSED\" if result.passed else \"‚ùå FAILED\"\n",
    "                print(f\"{status} - {result.details}\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"‚ùå Critical error running {strategy.test_name}: {str(e)}\")\n",
    "                error_result = TestResult(\n",
    "                    strategy.test_name, table_name, False, -1, -1,\n",
    "                    f\"Critical error: {str(e)}\", 0.0, self.data_provider.sample_mode\n",
    "                )\n",
    "                self.test_results.append(error_result)\n",
    "        \n",
    "        self._print_summary()\n",
    "        return self.test_results\n",
    "    \n",
    "    def _print_summary(self):\n",
    "        \"\"\"Print test execution summary\"\"\"\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"TEST SUMMARY\")\n",
    "        print(\"=\"*60)\n",
    "        \n",
    "        if not self.test_results:\n",
    "            print(\"No tests were run.\")\n",
    "            return\n",
    "        \n",
    "        total_tests = len(self.test_results)\n",
    "        passed_tests = sum(1 for result in self.test_results if result.passed)\n",
    "        failed_tests = total_tests - passed_tests\n",
    "        \n",
    "        print(f\"Total Tests: {total_tests}\")\n",
    "        print(f\"Passed: {passed_tests}\")\n",
    "        print(f\"Failed: {failed_tests}\")\n",
    "        print(f\"Success Rate: {(passed_tests/total_tests)*100:.1f}%\")\n",
    "        print()\n",
    "        \n",
    "        for result in self.test_results:\n",
    "            status = \"‚úÖ PASS\" if result.passed else \"‚ùå FAIL\"\n",
    "            print(f\"{status:8} | {result.execution_time_seconds:6.2f}s | {result.test_name:40} | {result.details}\")\n",
    "        \n",
    "        if failed_tests > 0:\n",
    "            print(f\"\\n‚ö†Ô∏è  {failed_tests} test(s) failed. Review details above.\")\n",
    "        else:\n",
    "            print(\"\\nüéâ All tests passed!\")\n",
    "\n",
    "def create_test_suite(spark_session: SparkSession, lakehouse_name) -> TestRunner:\n",
    "    data_provider = DataProvider(spark, lakehouse_name)\n",
    "    test_runner = TestRunner(data_provider)\n",
    "    return test_runner"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0317559-9d1d-42f8-ab71-dde04db22aaf",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "### **Modifiable**: Individual Test Strategy Definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b277e839-49b5-4919-a4e2-ba6b6e45ea2b",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.livy.statement-meta+json": {
       "execution_finish_time": "2025-08-18T18:45:11.8711396Z",
       "execution_start_time": "2025-08-18T18:45:11.5242443Z",
       "livy_statement_state": "available",
       "normalized_state": "finished",
       "parent_msg_id": "dbb0b915-df39-4ab7-aa64-716b7502eda4",
       "queued_time": "2025-08-18T18:45:11.5226563Z",
       "session_id": "099b5ced-d358-4fcb-b3a5-5180b5324e70",
       "session_start_time": null,
       "spark_pool": null,
       "state": "finished",
       "statement_id": 11,
       "statement_ids": [
        11
       ]
      },
      "text/plain": [
       "StatementMeta(, 099b5ced-d358-4fcb-b3a5-5180b5324e70, 11, Finished, Available, Finished)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def require_columns(df: DataFrame, cols: list[str | Column], where: str):\n",
    "    \"\"\"\n",
    "    Verify that all required columns exist in df.\n",
    "    \n",
    "    Args:\n",
    "        df: Spark DataFrame\n",
    "        cols: list of column names or Column objects\n",
    "        where: string describing where the check is happening (for error context)\n",
    "    \n",
    "    Raises:\n",
    "        ValueError if any required columns are missing\n",
    "    \"\"\"\n",
    "    # Normalize str|Column into names\n",
    "    normalized = []\n",
    "    for c in cols:\n",
    "        if isinstance(c, str):\n",
    "            normalized.append(c)\n",
    "        elif isinstance(c, Column):\n",
    "            # Spark Column often has an internal ._jc.toString()\n",
    "            # safer to let the caller alias() before passing\n",
    "            name = c._jc.toString() if hasattr(c, \"_jc\") else str(c)\n",
    "            normalized.append(name)\n",
    "        else:\n",
    "            raise TypeError(f\"Unsupported column type {type(c)} in require_columns\")\n",
    "\n",
    "    missing = [c for c in normalized if c not in df.columns]\n",
    "    if missing:\n",
    "        raise ValueError(\n",
    "            f\"{where}: missing columns {missing}. \"\n",
    "            f\"Available: {df.columns}\"\n",
    "        )\n",
    "\n",
    "class EffectiveDateValidityTest(TestStrategy):\n",
    "    \"\"\"Test that a given start date is less than or equal to a given end date\n",
    "    Used when intending to explode a sequence of date intervals\"\"\"\n",
    "\n",
    "    def __init__(self, id_col: str, start_col: str, end_col: str):\n",
    "        self.id_col = id_col\n",
    "        self.start_col = start_col\n",
    "        self.end_col = end_col\n",
    "    \n",
    "    @property\n",
    "    def test_name(self) -> str:\n",
    "        return f\"Date Validity ({self.start_col} <= {self.end_col})\"\n",
    "    \n",
    "    def execute(self, df: DataFrame, table_name: str, **kwargs) -> TestResult:\n",
    "        \"\"\"Execute an effective date validity test\n",
    "\n",
    "        kwargs:\n",
    "            table_name (str): The name of the table in the lakehouse\n",
    "            sampe_mode (bool): Whether or not to use random sampling to increase performance\n",
    "        \"\"\" \n",
    "        sample_mode = kwargs.get('sample_mode', False)\n",
    "        require_columns(df, [self.id_col, self.start_col, self.end_col], where = self.test_name)\n",
    "\n",
    "        builder = TestResultBuilder.for_test(self.test_name, table_name, sample_mode)\n",
    "\n",
    "        try:\n",
    "            total_count = df.count()\n",
    "            invalid_df = df.filter(F.col(self.start_col) > F.col(self.end_col))\n",
    "            error_count = invalid_df.count()\n",
    "            \n",
    "            details = f'Found {error_count} records with start_date > end_date out of {total_count} total records'\n",
    "            \n",
    "            if error_count > 0:\n",
    "                print(\"Sample of invalid records:\")\n",
    "                invalid_df.select(self.id_col, self.start_col, self.end_col).show(5)\n",
    "            \n",
    "            return builder.auto_pass_fail(total_count, error_count, details).end_timing().build()\n",
    "            \n",
    "        except Exception as e:\n",
    "            return builder.exception(e, \"Failed to validate effective dates\").end_timing().build()\n",
    "\n",
    "class UniquenessTest(TestStrategy):\n",
    "    \"\"\"Test whether each combination of id_cols is unique\"\"\"\n",
    "    def __init__(self, id_cols: list[str]):\n",
    "        self.id_cols = id_cols\n",
    "    \n",
    "    @property\n",
    "    def test_name(self) -> str:\n",
    "        return f\"Unique Columns, Subset: {self.id_cols}\"\n",
    "    \n",
    "    def execute(self, df: DataFrame, table_name: str, **kwargs):\n",
    "        sample_mode = kwargs.get('sample_mode', False)\n",
    "        require_columns(df, self.id_cols, where = self.test_name)\n",
    "\n",
    "        builder = TestResultBuilder.for_test(self.test_name, table_name, sample_mode)\n",
    "\n",
    "        try:\n",
    "            total_count = df.count()\n",
    "            duplicates = df.groupBy(self.id_cols).count().filter(col('count') > 1)\n",
    "            error_count = duplicates.count()\n",
    "\n",
    "            details = f'Found {error_count} duplicates of {total_count} total rows'\n",
    "\n",
    "            if error_count > 0:\n",
    "                print(f'Samples with duplicate {self.id_cols}')\n",
    "                duplicates.show(5)\n",
    "                builder.cache_failure(duplicates)\n",
    "            \n",
    "            \n",
    "            return builder.auto_pass_fail(total_count, error_count, details).end_timing().build()\n",
    "\n",
    "        except Exception as e:\n",
    "            return builder.exception(e, f\"Failed to test {self.id_cols} uniqueness\").end_timing().build()\n",
    "\n",
    "class DateContinuityTest(TestStrategy):\n",
    "    \"\"\"Tests that dates are contiguous with no gaps\"\"\"\n",
    "    def __init__(self, id_col: str | Column, date_col: str):\n",
    "        self.id_col = id_col\n",
    "        self.date_col = date_col\n",
    "    \n",
    "    @property\n",
    "    def test_name(self) -> str:\n",
    "        return f\"Date Column Continuity {self.date_col} by {self.id_col}\"\n",
    "    \n",
    "    def execute(self, df: DataFrame, table_name: str, **kwargs) -> TestResult:\n",
    "        sample_mode = kwargs.get('sample_mode')\n",
    "        # ID column isn't required in the df, since we may choose to group by a literal\n",
    "        require_columns(df, [self.date_col], where = self.test_name)\n",
    "\n",
    "        builder = TestResultBuilder.for_test(self.test_name, table_name, sample_mode)\n",
    "\n",
    "        try:\n",
    "            id_alias = '_id'\n",
    "            id_column = (F.col(self.id_col) if isinstance(self.id_col, str) else self.id_col).alias(id_alias)\n",
    "            date_column = F.col(self.date_col) if isinstance(self.date_col, str) else self.date_col\n",
    "\n",
    "            total_by_id = df.select(id_column).distinct().count()\n",
    "            \n",
    "            unit_date_ranges = (df.groupBy(id_column)\n",
    "                               .agg(\n",
    "                                   F.min(date_column).alias('min_date'),\n",
    "                                   F.max(date_column).alias('max_date'),\n",
    "                                   F.count(date_column).alias('actual_count')\n",
    "                               ))\n",
    "\n",
    "            \n",
    "            unit_expected = unit_date_ranges.withColumn(\n",
    "                \"expected_count\",\n",
    "                F.datediff(F.col('max_date'), F.col('min_date')) + 1\n",
    "            )\n",
    "            \n",
    "            gaps_df = unit_expected.filter(F.col('actual_count') < F.col('expected_count'))\n",
    "            error_count = gaps_df.count()\n",
    "            \n",
    "            details = f'Found {error_count} {id_alias}s with date gaps out of {total_by_id} {self.id_col}s'\n",
    "            \n",
    "            if error_count > 0:\n",
    "                print(f\"Sample {self.id_col}s with gaps:\")\n",
    "                \n",
    "            return builder.auto_build(total_by_id, error_count, details)\n",
    "        except Exception as e:\n",
    "            return builder.exception(e, f\"Failed to test date continuity for {self.date_col}\").end_timing().build()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "139f4e6f-a61a-44db-bbb1-a6502cd7660d",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    },
    "nteract": {
     "transient": {
      "deleting": false
     }
    }
   },
   "source": [
    "## **Usage Examples**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b59fca8a-2298-4009-ad0d-d33275e02873",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.livy.statement-meta+json": {
       "execution_finish_time": "2025-08-18T18:45:25.9322844Z",
       "execution_start_time": "2025-08-18T18:45:14.09697Z",
       "livy_statement_state": "available",
       "normalized_state": "finished",
       "parent_msg_id": "b16ae000-bb42-4e01-a291-47ecdc481b2f",
       "queued_time": "2025-08-18T18:45:14.0956564Z",
       "session_id": "099b5ced-d358-4fcb-b3a5-5180b5324e70",
       "session_start_time": null,
       "spark_pool": null,
       "state": "finished",
       "statement_id": 12,
       "statement_ids": [
        12
       ]
      },
      "text/plain": [
       "StatementMeta(, 099b5ced-d358-4fcb-b3a5-5180b5324e70, 12, Finished, Available, Finished)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enabling sample mode with 10 properties...\n",
      "Sample mode enabled with 10 properties\n",
      "Added test: Date Validity (EFFECTIVE_START_DATE <= EFFECTIVE_END_DATE) for FACT_UNIT_EVENT to queue\n",
      "Added test: Unique Columns, Subset: ['UNIT_SPACE_UUID', 'DATE'] for FACT_UNIT_EVENT_DAILY to queue\n",
      "Added test: Date Column Continuity DATE by UNIT_SPACE_UUID for FACT_UNIT_EVENT_DAILY to queue\n",
      "Added test: Date Column Continuity DATE_KEY by Column<'1'> for DIM_DATE to queue\n"
     ]
    }
   ],
   "source": [
    "# Spark will be defined in the fabric environment.\n",
    "test_runner = create_test_suite(spark, 'TRINITY_BI')\n",
    "test_prop_ids = test_runner.data_provider.enable_sample_mode(10)\n",
    "\n",
    "# Define the individual test strategies\n",
    "effective_date_valid = EffectiveDateValidityTest(\n",
    "    id_col = 'UNIT_SPACE_UUID',\n",
    "    start_col = 'EFFECTIVE_START_DATE',\n",
    "    end_col = 'EFFECTIVE_END_DATE'\n",
    ")\n",
    "\n",
    "ued_date_uniquess = UniquenessTest(id_cols = ['UNIT_SPACE_UUID', 'DATE'])\n",
    "ued_date_continuity = DateContinuityTest(id_col = 'UNIT_SPACE_UUID', date_col = 'DATE')\n",
    "dim_date_continuity = DateContinuityTest(id_col = lit(1), date_col = 'DATE_KEY') #lit(1) used for no col dependency\n",
    "\n",
    "# Queue the individual test cases\n",
    "test_runner.add_test(effective_date_valid, 'FACT_UNIT_EVENT')\n",
    "test_runner.add_test(ued_date_uniquess, 'FACT_UNIT_EVENT_DAILY')\n",
    "test_runner.add_test(ued_date_continuity, 'FACT_UNIT_EVENT_DAILY')\n",
    "test_runner.add_test(dim_date_continuity, 'DIM_DATE', disable_sampling = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "699bc987-15b0-4550-8fb4-90149be3ad9a",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.livy.statement-meta+json": {
       "execution_finish_time": "2025-08-18T18:45:57.5020196Z",
       "execution_start_time": "2025-08-18T18:45:52.7450453Z",
       "livy_statement_state": "available",
       "normalized_state": "finished",
       "parent_msg_id": "c45a9eba-32a6-4669-afbd-31526b06ce4f",
       "queued_time": "2025-08-18T18:45:52.7438275Z",
       "session_id": "099b5ced-d358-4fcb-b3a5-5180b5324e70",
       "session_start_time": null,
       "spark_pool": null,
       "state": "finished",
       "statement_id": 13,
       "statement_ids": [
        13
       ]
      },
      "text/plain": [
       "StatementMeta(, 099b5ced-d358-4fcb-b3a5-5180b5324e70, 13, Finished, Available, Finished)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test queue cleared\n",
      "Added test: Date Column Continuity DATE_KEY by Column<'1'> for DIM_DATE to queue\n",
      "============================================================\n",
      "STARTING LAKEHOUSE DATA TESTING SUITE\n",
      "Mode: SAMPLE\n",
      "Sample Properties: 10\n",
      "Queued Tests: 1\n",
      "============================================================\n",
      "\n",
      "üîÑ Running Date Column Continuity DATE_KEY by Column<'1'> on DIM_DATE...\n",
      "‚ö†Ô∏è  Sampling disabled for DIM_DATE: no 'PROPERTY_UUID' column.\n",
      "‚úÖ PASSED - Found 0 _ids with date gaps out of 1 Column<'1'>s\n",
      "\n",
      "============================================================\n",
      "TEST SUMMARY\n",
      "============================================================\n",
      "Total Tests: 1\n",
      "Passed: 1\n",
      "Failed: 0\n",
      "Success Rate: 100.0%\n",
      "\n",
      "‚úÖ PASS   |   3.63s | Date Column Continuity DATE_KEY by Column<'1'> | Found 0 _ids with date gaps out of 1 Column<'1'>s\n",
      "\n",
      "üéâ All tests passed!\n"
     ]
    }
   ],
   "source": [
    "# NOTE: All tests are only queued, so they can be ran in other notebooks\n",
    "# to run all queued tests \n",
    "# results = test_runner.run_all_tests()\n",
    "#resultsDf = spark.createDataFrame([r.to_dict() for r in results])"
   ]
  }
 ],
 "metadata": {
  "dependencies": {
   "lakehouse": {
    "default_lakehouse": "d807aa2a-c5e0-4478-880f-9589db908321",
    "default_lakehouse_name": "TRINITY_BI",
    "default_lakehouse_workspace_id": "f1c3fca4-8cc5-4823-a1f7-ef2932046def",
    "known_lakehouses": [
     {
      "id": "d807aa2a-c5e0-4478-880f-9589db908321"
     }
    ]
   }
  },
  "kernel_info": {
   "name": "synapse_pyspark"
  },
  "kernelspec": {
   "display_name": "synapse_pyspark",
   "name": "synapse_pyspark"
  },
  "language_info": {
   "name": "python"
  },
  "microsoft": {
   "language": "python",
   "language_group": "synapse_pyspark",
   "ms_spell_check": {
    "ms_spell_check_language": "en"
   }
  },
  "nteract": {
   "version": "nteract-front-end@1.0.0"
  },
  "spark_compute": {
   "compute_id": "/trident/default",
   "session_options": {
    "conf": {
     "spark.synapse.nbs.session.timeout": "1200000"
    }
   }
  },
  "synapse_widget": {
   "state": {},
   "version": "0.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
